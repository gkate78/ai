{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required packages\n",
    "- `%%capture` is used to suppress the output of the installation commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install llama-index-readers-file pymupdf\n",
    "! pip install llama-index-vector-stores-postgres\n",
    "! pip install llama-index-embeddings-huggingface\n",
    "! pip install psycopg2-binary\n",
    "! pip install ipywidgets\n",
    "! pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the embedding model\n",
    "- Here we setup the embedding model to use and get the size of the embedding to pass to PgVector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04875882342457771, -0.04734064266085625, 0.020610108971595764, 0.02316340245306492, 0.04693278670310974]\n",
      "Emedding length: 768\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\"\"\"\n",
    "Get embeddings using Hugging Face embeddings (can be local or remote)\n",
    "Note: we use this because the OpenAI API embedding model cannot be used in Llama Index\n",
    "\"\"\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "text_embedding = embedding_model.get_text_embedding(\"Once upon a time, there was a cat.\")\n",
    "print(text_embedding[:5])\n",
    "print(f\"Emedding length: {len(text_embedding)}\")\n",
    "vector_size = len(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the LLM\n",
    "- Here we setup the LLM hosted through LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.lmstudio import LMStudio\n",
    "\n",
    "llm = LMStudio(\n",
    "    model_name=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    temperature=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup PgVector extension in Postgres SQL\n",
    "- In the code below, we drop the database everytime, just to ensure that we are starting from scratch. This is not recommended in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nest_asyncio\n",
    "\n",
    "try:\n",
    "    pg_pw = \"mysecretpassword\"\n",
    "    pg_db = \"vector_store\"\n",
    "    connection_string = f\"postgresql://postgres:{pg_pw}@localhost:5432\"\n",
    "    db_name = pg_db\n",
    "    conn = psycopg2.connect(connection_string)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    with conn.cursor() as c:\n",
    "        c.execute(f\"DROP DATABASE {db_name} WITH (FORCE);\")\n",
    "        c.execute(f\"CREATE DATABASE {db_name};\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the PDF/s we want to ingest\n",
    "- We download the PDF/s we want to ingest and store them in the `data` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-11 12:50:06--  https://arxiv.org/pdf/2401.05856.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/2401.05856 [following]\n",
      "--2024-07-11 12:50:06--  http://arxiv.org/pdf/2401.05856\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 570647 (557K) [application/pdf]\n",
      "Saving to: ‘data/RAG_Failure_Points.pdf’\n",
      "\n",
      "data/RAG_Failure_Po 100%[===================>] 557.27K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-07-11 12:50:07 (5.31 MB/s) - ‘data/RAG_Failure_Points.pdf’ saved [570647/570647]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir data\n",
    "! wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2401.05856.pdf\" -O \"data/RAG_Failure_Points.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF data file ingestion\n",
    "- we configure LlamaIndex's Settings global configuration with the models we want to use, and the chunk size.\n",
    "- We ingest the PDF data file into the database using the `pgvector` extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# Configuring the LLM (LLM2)\n",
    "Settings.llm = llm\n",
    "\n",
    "# Configuring the embedding model (LLM1)\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.chunk_size = 768\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "BASE_DIR = \"./data\"\n",
    "\n",
    "def simple_RAG(vector_size):\n",
    "    \"\"\"\n",
    "    Simple Retrieval Augmented Generation (RAG) using Llama Index.\n",
    "    \"\"\"\n",
    "\n",
    "    url = make_url(connection_string)\n",
    "    print(f\"Url {url}\")\n",
    "    \n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=db_name,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port,\n",
    "        user=url.username,\n",
    "        table_name=\"basic_rag\",\n",
    "        embed_dim=vector_size\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    nodes = ingest_documents(BASE_DIR)\n",
    "    \n",
    "    print(f\"Number of nodes: {len(nodes)}\")\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(nodes, storage_context=storage_context, show_progress=True)\n",
    "    return index\n",
    "\n",
    "def ingest_documents(directory):\n",
    "    \"\"\"\n",
    "    Ingest documents from a directory into the vector store. \n",
    "    \"\"\"\n",
    "    reader = SimpleDirectoryReader(input_dir=directory)\n",
    "    return reader.load_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the ingestion\n",
    "- now run the data ingestion\n",
    "- also setup the query engines including the streaming version of the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url postgresql://postgres:***@localhost:5432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00,  5.29file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3851a18996f49b685bb6504e90e60a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793802f1cbba4209a61e2878cb29d938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = simple_RAG(vector_size=vector_size)\n",
    "query_engine = index.as_query_engine(verbose=True)\n",
    "streaming_query_engine = index.as_query_engine(verbose=True, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the data\n",
    "- we query the data, then pretty print the results\n",
    "- also show the node source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: FP1: Missing Content  The first failure point occurs\n",
      "when asking a question that cannot be answered from the available\n",
      "documents. In this scenario, the RAG system will respond with\n",
      "something like \"Sorry, I don't know\". However, for questions related\n",
      "to the content but without answers, the system could be fooled into\n",
      "giving a response.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 07b3ecd1-ecd5-4ab6-a4cd-25b28b493a0a\n",
      "Similarity: 0.6897327077707449\n",
      "Text: CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus\n",
      "Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek Case\n",
      "Study Domain Doc Types Dataset Size RAG Stages Sample Questions\n",
      "Cognitive Reviewer*Research PDFs (Any size) Chunker, Rewriter, Re-\n",
      "triever, ReaderWhat are the key points covered in this paper? AI\n",
      "Tutor* Education V...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 3b683016-1673-49d1-be35-d7af8e9f03dc\n",
      "Similarity: 0.6517672953883104\n",
      "Text: Seven Failure Points When Engineering a Retrieval Augmented\n",
      "Generation System CAIN 2024, April 2024, Lisbon, Portugal Figure 1:\n",
      "Indexing and Query processes required for creating a Retrieval\n",
      "Augmented Generation (RAG) system. The indexing process is typically\n",
      "done at development time and queries at runtime. Failure points\n",
      "identified in this stud...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "response = query_engine.query(\"What is the first RAG Failure Point? Please provide a brief description.\")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the data using the streaming query engine\n",
    "- we query the data using the streaming query engine, then print the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third RAG failure point is \"Not in Context - Consolidation strategy Limitations\". This refers to a situation where documents with the correct answers are retrieved from the database, but do not make it into the context for generating an answer. This occurs when many documents are returned from the database and a consolidation process takes place to retrieve the answer."
     ]
    }
   ],
   "source": [
    "streaming_response = streaming_query_engine.query(\"What is the third RAG Failure Point? Please provide a brief description.\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about the prompts?\n",
    "- By default, the prompts are already set to default values. You can change them to your liking.\n",
    "- Below we are displaying the default prompts.\n",
    "- Assignment: Investigate how to change the default prompts and see how it affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "        \n",
    "\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of the Basic RAG Process\n",
    "\n",
    "![Review of the Basic RAG Process](./images/4-basic-rag-system.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
